{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "eps = 10**-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"output/sorted_train0310.csv\")\n",
    "# test = pd.read_csv(\"output/sorted_test0310.csv\")\n",
    "# val = pd.read_csv(\"output/sorted_val0310.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BorrCity BorrState BorrZip        CDC_City CDC_State  CDC_Zip\n",
      "0    SAN JOSE        CA   95111   San Francisco        CA  94133.0\n",
      "1   KENNEWICK        WA   99336  Spokane Valley        WA  99212.0\n",
      "2    KIRKLAND        WA   98033         Seattle        WA  98168.0\n",
      "3    STAMFORD        CT    6902         Danbury        CT   6810.0\n",
      "4  WALKERTOWN        NC   27051    Kernersville        NC  27284.0\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"output/random_train0310.csv\")\n",
    "test = pd.read_csv(\"output/random_test0310.csv\")\n",
    "val = pd.read_csv(\"output/random_val0310.csv\")\n",
    "\n",
    "train = train[train.LoanStatus != \"MISSING\"]\n",
    "test = test[test.LoanStatus != \"MISSING\"]\n",
    "val = val[val.LoanStatus != \"MISSING\"]\n",
    "\n",
    "print train.head()[train.columns[:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "Chargeoffs: 7124\n",
      "Percentage of loans that defaulted:"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'm_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-84b4d5682178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchargeoff_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoanStatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CHGOFF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Chargeoffs:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchargeoff_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Percentage of loans that defaulted:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchargeoff_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test set:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute some statistics\n",
    "print 'Train set:'\n",
    "chargeoff_count = (train.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_train\n",
    "\n",
    "print '\\n', 'Test set:'\n",
    "chargeoff_count = (test.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_test\n",
    "\n",
    "print '\\n', 'Validation set:'\n",
    "chargeoff_count = (val.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = float(len(train))\n",
    "m_test = float(len(test))\n",
    "m_val = float(len(val))\n",
    "print m_train, m_test, m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for the columns you don't want to include for your model\n",
    "c = train.columns.tolist()\n",
    "c.remove('LoanStatus')\n",
    "c.remove('ChargeOffDate')\n",
    "c.remove('GrossChargeOffAmount')\n",
    "c.remove('BorrZip')\n",
    "c.remove('CDC_Zip')\n",
    "c.remove('BorrCity')\n",
    "c.remove('CDC_City')\n",
    "c.remove('ThirdPartyLender_City')\n",
    "# c.remove('BorrState')\n",
    "# c.remove('CDC_State')\n",
    "# c.remove('ThirdPartyLender_State')\n",
    "c.remove('ProjectCounty')\n",
    "c.remove('ProjectState')\n",
    "c.remove('ApprovalDate')\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[c]\n",
    "x_test = test[c]\n",
    "x_val = val[c]\n",
    "\n",
    "y_train_logit = train['LoanStatus'].values == \"CHGOFF\"\n",
    "y_test_logit = test['LoanStatus'].values == \"CHGOFF\"\n",
    "y_val_logit = val['LoanStatus'].values == \"CHGOFF\"\n",
    "\n",
    "print x_train.head()[x_train.columns[:4]]\n",
    "print '\\n', y_train_logit[:5]\n",
    "\n",
    "# hazard_target = train['ChargeOffDate']\n",
    "# amount_target = train['GrossChargeOffAmount']\n",
    "\n",
    "# print train.dtypes\n",
    "# convert categorical variables to dummy variables\n",
    "x_train = pd.get_dummies(x_train)\n",
    "x_test = pd.get_dummies(x_test)\n",
    "x_val = pd.get_dummies(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get consistent feature dimensions for both train and test dataset\n",
    "def add_missing_dummy_columns(d1, d2):\n",
    "    missing_cols = set(d1.columns) - set(d2.columns)\n",
    "    for c in missing_cols:\n",
    "        d2[c] = 0\n",
    "    return d2\n",
    "\n",
    "print 'before fixing columns: '\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "print x_val.shape\n",
    "\n",
    "def fix_columns(x_train, x_test):  \n",
    "\n",
    "    x_test = add_missing_dummy_columns(x_train, x_test)\n",
    "\n",
    "    extra_cols = set(x_test.columns) - set(x_train.columns)\n",
    "    x_test = x_test[x_train.columns]\n",
    "    return x_test\n",
    "\n",
    "x_test = fix_columns(x_train, x_test)\n",
    "x_val = fix_columns(x_train, x_val)\n",
    "print 'after fixing columns: '\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "print x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to models \n",
    "# below is to fit to logit model\n",
    "\n",
    "# y_train_logit = np.reshape(l, (len(l),))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "solver = 'liblinear'\n",
    "\n",
    "lr = LogisticRegression(solver=solver, \n",
    "                        multi_class= 'ovr', \n",
    "                        C=50.,\n",
    "                        penalty = 'l2',\n",
    "                        fit_intercept=True,\n",
    "                        max_iter=200,\n",
    "                        random_state=None, \n",
    "                       )\n",
    "lr.fit(x_train, y_train_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_val = lr.predict_proba(x_val)\n",
    "print y_probs_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_val = y_probs_val[:,1] > 0.2\n",
    "print y_preds_val[:5].reshape(5,1)\n",
    "print \"\\n\", \"validation accuracy\", np.sum(y_preds_val == y_val_logit, dtype=float) / m_val\n",
    "print \"false positive rate\",  np.sum((y_val_logit == 0) * (y_preds_val == 1)) / (np.sum(y_val_logit == 0, dtype=float) + eps)\n",
    "print \"true positive rate\", np.sum((y_val_logit == 1) * (y_preds_val == 1)) / (np.sum(y_val_logit == 1, dtype=float) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fprs = []\n",
    "tprs = []\n",
    "step_size = 0.0001\n",
    "for threshold in np.arange(0.001,1,step_size):\n",
    "    y_preds_val = y_probs_val[:,1] > threshold\n",
    "    num_pos = np.sum(y_preds_val == 1, dtype=float) + eps\n",
    "    fprs.append(np.sum((y_val_logit == 0) * (y_preds_val == 1)) / (np.sum(y_val_logit == 0, dtype=float) + eps))\n",
    "    tprs.append(np.sum((y_val_logit == 1) * (y_preds_val == 1)) / (np.sum(y_val_logit == 1, dtype=float) + eps))\n",
    "\n",
    "plt.plot(fprs,tprs)\n",
    "plt.title(\"ROC curve for Logistic Regression\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prev = 0.000\n",
    "auc = 0\n",
    "for x, y in zip(sorted(fprs), sorted(tprs)):\n",
    "    auc += (x - x_prev)*y\n",
    "    x_prev = x\n",
    "print \"area under curve: \", auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses = np.array(tprs, dtype=float) + eps\n",
    "specs = 1 - np.array(fprs, dtype=float) + eps\n",
    "h_mean = 2. / (1 / senses + 1 / specs)\n",
    "opt_index = np.argmax(h_mean)\n",
    "opt_threshold = opt_index*step_size\n",
    "\n",
    "print \"best threshold: \" , opt_threshold\n",
    "print \"highest sensitivity: \", tprs[opt_index]\n",
    "print \"highest specificity: \", 1 - fprs[opt_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_train = lr.predict_proba(x_train)[:,1]\n",
    "y_preds_train = y_probs_train > opt_threshold\n",
    "\n",
    "y_probs_test = lr.predict_proba(x_test)[:,1]\n",
    "y_preds_test = y_probs_test > opt_threshold\n",
    "print y_probs_test[:5]\n",
    "print y_preds_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on training data to see if there is underfitting\n",
    "print 'Training set'\n",
    "print 'predicted proportion of defaults: ', np.sum(y_preds_train == 1) / m_train\n",
    "print 'accuracy: ', np.sum(y_preds_train == y_train_logit) / m_train\n",
    "\n",
    "# fit on test set to see if there is overfitting\n",
    "print '\\nTest set'\n",
    "print 'predicted proportion of defaults: ', np.sum(y_preds_test == 1) / m_test\n",
    "print 'accuracy: ', np.sum(y_preds_test == y_test_logit) / m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to discuss/redo cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# print 'Training set'\n",
    "# print metrics.classification_report(y_train_logit, y_pred_train)\n",
    "# print '\\nTest set'\n",
    "# print metrics.classification_report(y_test_logit, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### optional ##########\n",
    "# # if import from sorted data - need to downsample train dataset to be more balanced\n",
    "# # can call the following function multiple times for decreasing in log term\n",
    "# # dropping PIF samples with 50% probability\n",
    "\n",
    "# def drop_half(train):\n",
    "#     print 'before drop: ', train.shape[0]\n",
    "#     dropidx = []\n",
    "#     for idx, val in train['LoanStatus'].iteritems():\n",
    "#         if val != 'CHGOFF':\n",
    "#             rand = np.random.rand()\n",
    "#             if rand < 0.5:\n",
    "#                 dropidx.append(idx)\n",
    "#     train = train.drop(dropidx)\n",
    "#     print 'after drop: ', train.shape[0]\n",
    "#     return train\n",
    "    \n",
    "# train = drop_half(train)\n",
    "# train = drop_half(train)\n",
    "# # test = drop_half(test)\n",
    "\n",
    "# chargeoff_count = (train['LoanStatus'] == \"CHGOFF\").sum()\n",
    "# print 'Chargeoffs:', chargeoff_count\n",
    "# print 'Percentage of loans that defaulted:', float(chargeoff_count)/len(train)\n",
    "\n",
    "# chargeoff_count = (test['LoanStatus'] == \"CHGOFF\").sum()\n",
    "# print 'Chargeoffs:', chargeoff_count\n",
    "# print 'Percentage of loans that defaulted:', float(chargeoff_count)/len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
