{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "step_size = 0.0001\n",
    "penalty = 'l2'\n",
    "eps = 10**-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 columns\n",
      "BorrCity                              object\n",
      "BorrState                             object\n",
      "BorrZip                               object\n",
      "CDC_City                              object\n",
      "CDC_State                             object\n",
      "CDC_Zip                               object\n",
      "ThirdPartyLender_City                 object\n",
      "ThirdPartyLender_State                object\n",
      "GrossApproval                        float64\n",
      "ApprovalDate                          object\n",
      "ApprovalFiscalYear                     int64\n",
      "DeliveryMethod                        object\n",
      "subpgmdesc                            object\n",
      "TermInMonths                           int64\n",
      "ProjectCounty                         object\n",
      "ProjectState                          object\n",
      "BusinessType                          object\n",
      "LoanStatus                            object\n",
      "ChargeOffDate                         object\n",
      "GrossChargeOffAmount                 float64\n",
      "SP500_Yearly_Return                  float64\n",
      "CPI                                  float64\n",
      "Yearly_Unemployment_Rate             float64\n",
      "Log_GrossApproval_Norm               float64\n",
      "Log_Yearly_Unemployment_Rate_Norm    float64\n",
      "Log_HPI_Norm                         float64\n",
      "ThirdPartyDollars_Norm               float64\n",
      "TermMultipleYear                        bool\n",
      "RepeatBorrower                         int64\n",
      "BankStateneqBorrowerState               bool\n",
      "ProjectStateneqBorrowerState            bool\n",
      "2DigitNaics                           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train = pd.read_csv(\"data/random_test0314.csv\")\n",
    "test = pd.read_csv(\"data/random_train0314.csv\")\n",
    "val = pd.read_csv(\"data/random_val0314.csv\")\n",
    "\n",
    "print len(train.columns), \"columns\"\n",
    "print train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = float(len(train))\n",
    "m_test = float(len(test))\n",
    "m_val = float(len(val))\n",
    "print m_train, m_test, m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some statistics\n",
    "print 'Train set:'\n",
    "chargeoff_count = (train.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_train\n",
    "\n",
    "print '\\n', 'Test set:'\n",
    "chargeoff_count = (test.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_test\n",
    "\n",
    "print '\\n', 'Validation set:'\n",
    "chargeoff_count = (val.LoanStatus == \"CHGOFF\").sum()\n",
    "print 'Chargeoffs:', chargeoff_count\n",
    "print 'Percentage of loans that defaulted:', float(chargeoff_count)/m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables we don't want\n",
    "c = train.columns.tolist()\n",
    "dropped_columns = ['LoanStatus', 'ChargeOffDate', 'GrossChargeOffAmount', 'BorrZip', 'CDC_Zip', 'BorrCity',\n",
    "                   'CDC_City', 'ThirdPartyLender_City', 'ProjectCounty', 'ApprovalDate', 'GrossApproval']\n",
    "for col in dropped_columns:\n",
    "    c.remove(col)\n",
    "print(len(c)), \"covariates\"\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data into covariates and targets\n",
    "x_train = train[c]\n",
    "x_test = test[c]\n",
    "x_val = val[c]\n",
    "\n",
    "y_train = (train['LoanStatus'].values == \"CHGOFF\")*1\n",
    "y_test = (test['LoanStatus'].values == \"CHGOFF\")*1\n",
    "y_val = (val['LoanStatus'].values == \"CHGOFF\")*1\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate how many covariates we add by the expanding categorical columns\n",
    "numerics = x_train.select_dtypes(include=[np.number,'bool'])\n",
    "cats = x_train.drop(columns=numerics.columns)\n",
    "\n",
    "uniques = np.unique(cats.values)\n",
    "\n",
    "print \"approx. number of covariate levels:\", len(uniques)\n",
    "print cats.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variables to dummy variables\n",
    "x_train = pd.get_dummies(x_train)\n",
    "x_test = pd.get_dummies(x_test)\n",
    "x_val = pd.get_dummies(x_val)\n",
    "\n",
    "print x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the data sets all have the same levels\n",
    "def add_missing_columns(d1, d2):\n",
    "    missing_cols_2 = (set(d1.columns) - set(d2.columns))\n",
    "    missing_cols_1 = (set(d2.columns) - set(d1.columns))\n",
    "    for c in missing_cols_2:\n",
    "        d2[c] = 0\n",
    "    for c in missing_cols_1:\n",
    "        d1[c] = 0\n",
    "    return d1, d2\n",
    "\n",
    "print 'before fixing columns: '\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "print x_val.shape\n",
    "\n",
    "x_train, x_test = add_missing_columns(x_train, x_test)\n",
    "x_train, x_val = add_missing_columns(x_train, x_val)\n",
    "x_test, x_val = add_missing_columns(x_test, x_val)\n",
    "print 'after fixing columns: '\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "print x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best regularization factor and stopping time\n",
    "def lr_model(x, y, C = 1, penalty = 'l2', tol = 10**-4):\n",
    "    lr = LogisticRegression(solver='liblinear', multi_class= 'ovr', C=C,\n",
    "                        penalty = penalty, fit_intercept=True, max_iter=200, tol = tol)\n",
    "    lr.fit(x, y)\n",
    "    return lr\n",
    "\n",
    "def calc_auc(x, y, C = 1, penalty = 'l2', tol = 10**-4):\n",
    "    lr = lr_model(x, y, C = C, penalty = penalty, tol = tol)\n",
    "    \n",
    "    y_probs_val = lr.predict_proba(x_val)\n",
    "    val_fprs, val_tprs  = [], [] \n",
    "    for threshold in np.arange(step_size,1,step_size):\n",
    "        y_preds_val = y_probs_val[:,1] > threshold\n",
    "        val_fprs.append(np.sum((y_val == 0) * (y_preds_val == 1)) / (np.sum(y_val == 0, dtype=float) + eps))\n",
    "        val_tprs.append(np.sum((y_val == 1) * (y_preds_val == 1)) / (np.sum(y_val == 1, dtype=float) + eps))\n",
    "\n",
    "    x_prev = 0.\n",
    "    auc = 0\n",
    "    for x, y in zip(sorted(val_fprs), sorted(val_tprs)):\n",
    "        auc += (x - x_prev)*y\n",
    "        x_prev = x\n",
    "    return auc\n",
    "\n",
    "C_range = [2**i for i in range(-5,5)]\n",
    "tol_range = [10**i for i in range(-8, -1)]\n",
    "auc_results = {}\n",
    "for C in C_range:\n",
    "    for tol in tol_range:\n",
    "        auc_results[calc_auc(x_train, y_train, C = C, tol = tol)] = (C, tol)\n",
    "opt_auc = np.max(auc_results.keys())\n",
    "opt_setting = auc_results[opt_auc]\n",
    "\n",
    "print opt_setting, \":\", opt_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC curves\n",
    "auc_C_avg = np.zeros(len(C_range))\n",
    "auc_tol_avg = np.zeros(len(tol_range))\n",
    "for auc, (C, tol) in auc_results.items():\n",
    "    auc_C_avg[C_range.index(C)] += auc\n",
    "    auc_tol_avg[tol_range.index(tol)] += auc\n",
    "auc_C_avg = auc_C_avg / len(tol_range)\n",
    "auc_tol_avg = auc_tol_avg / len(C_range)\n",
    "\n",
    "plt.xscale('log', basex = 2)\n",
    "plt.plot(C_range, auc_C_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log', basex = 10)\n",
    "plt.plot(tol_range, auc_tol_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model and generate outputs\n",
    "C_opt, tol_opt = opt_setting\n",
    "lr = lr_model(x_train, y_train, C = C_opt, penalty = penalty, tol = tol_opt)\n",
    "\n",
    "y_probs_train = lr.predict_proba(x_train)\n",
    "y_probs_val = lr.predict_proba(x_val)\n",
    "print y_probs_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate some statistics\n",
    "y_preds_val = y_probs_val[:,1] > 0.2\n",
    "print y_preds_val[:5].reshape(5,1)\n",
    "print \"\\n\", \"validation accuracy\", np.sum(y_preds_val == y_val, dtype=float) / m_val\n",
    "print \"false positive rate\",  np.sum((y_val == 0) * (y_preds_val == 1)) / (np.sum(y_val == 0, dtype=float) + eps)\n",
    "print \"true positive rate\", np.sum((y_val == 1) * (y_preds_val == 1)) / (np.sum(y_val == 1, dtype=float) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ROC curve\n",
    "train_fprs, train_tprs, val_fprs, val_tprs  = [], [], [], []\n",
    "for threshold in np.arange(step_size,1,step_size):\n",
    "    y_preds_val = y_probs_val[:,1] > threshold\n",
    "    val_fprs.append(np.sum((y_val == 0) * (y_preds_val == 1)) / (np.sum(y_val == 0) + eps))\n",
    "    val_tprs.append(np.sum((y_val == 1) * (y_preds_val == 1)) / (np.sum(y_val == 1) + eps))\n",
    "    \n",
    "    y_preds_train = y_probs_train[:,1] > threshold\n",
    "    train_fprs.append(np.sum((y_train == 0) * (y_preds_train == 1)) / (np.sum(y_train == 0) + eps))\n",
    "    train_tprs.append(np.sum((y_train == 1) * (y_preds_train == 1)) / (np.sum(y_train == 1) + eps))\n",
    "print val_fprs[:5]\n",
    "print val_tprs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "plt.title(\"ROC Curve for Logistic Regression\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.xlabel(\"false positive rate\")\n",
    "\n",
    "valplt, = plt.plot(val_fprs,val_tprs,label='validation')\n",
    "trainplt, = plt.plot(train_fprs,train_tprs,label='train')\n",
    "plt.legend(handles = [valplt, trainplt])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = 0\n",
    "x_prev = 0\n",
    "for x, y in zip(sorted(val_fprs), sorted(val_tprs)):\n",
    "    auc += (x - x_prev)*y\n",
    "    x_prev = x\n",
    "print auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ROC curve\n",
    "with open(\"logistic_roc.csv\", \"w\") as file:\n",
    "    file.write(\"threshold,fpr,tpr\\n\")\n",
    "    for i, (fpr, tpr) in enumerate(zip(val_fprs, val_tprs), 1):\n",
    "        file.write(str(step_size + i) + \",\" + str(fpr) + \",\" + str(tpr) + \"\\n\")\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best threshold\n",
    "senses = np.array(val_tprs, dtype=float) + eps\n",
    "specs = 1 - np.array(val_fprs, dtype=float) + eps\n",
    "h_mean = 2. / (1 / senses + 1 / specs)\n",
    "opt_index = np.argmax(h_mean)\n",
    "opt_threshold = opt_index*step_size\n",
    "\n",
    "print \"best threshold: \" , opt_threshold\n",
    "print \"highest sensitivity: \", val_tprs[opt_index]\n",
    "print \"highest specificity: \", 1 - val_fprs[opt_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_probs_train = lr.predict_proba(x_train)[:,1]\n",
    "y_preds_train = y_probs_train > opt_threshold\n",
    "\n",
    "y_probs_test = lr.predict_proba(x_test)[:,1]\n",
    "y_preds_test = y_probs_test > opt_threshold\n",
    "\n",
    "print y_probs_test[:5]\n",
    "print y_preds_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on training data to see if there is underfitting\n",
    "print 'Training set'\n",
    "print 'predicted proportion of defaults: ', np.sum(y_preds_train == 1) / m_train\n",
    "print 'accuracy: ', np.sum(y_preds_train == y_train) / m_train\n",
    "\n",
    "# fit on test set to see if there is overfitting\n",
    "print '\\nTest set'\n",
    "print 'predicted proportion of defaults: ', np.sum(y_preds_test == 1) / m_test\n",
    "print 'accuracy: ', np.sum(y_preds_test == y_test) / m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "precision = np.sum((y_test == 1) * (y_preds_test == 1)) / (np.sum(y_preds_test == 1) + eps)\n",
    "recall = np.sum((y_test == 1) * (y_preds_test == 1)) / (np.sum(y_test == 1) + eps)\n",
    "F1 = 2/(1 / (precision + eps) + 1 / (recall + eps))\n",
    "print \"precision: \", round(precision,6), \"\\trecall:\", round(recall,6), \"\\tF1 score:\", round(F1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood estimation\n",
    "def log_likelihood(probs, y):\n",
    "    return np.sum(np.log(probs[y*1]))\n",
    "\n",
    "full_ll = log_likelihood(y_probs_train, y_train)\n",
    "\n",
    "ll_ratio_test = []\n",
    "for column in x_train.columns:\n",
    "    x_temp = x_train.copy()\n",
    "    x_temp[column] = 0\n",
    "    y_probs_temp = lr.predict_proba(x_temp)\n",
    "    temp_ll = log_likelihood(y_probs_temp, y_train)\n",
    "    ll_ratio_test.append((column,2*(full_ll - temp_ll)))\n",
    "\n",
    "ll_ratio_test = sorted(ll_ratio_test, key=lambda t: (-t[1], t[0]))\n",
    "\n",
    "for i in range(5):\n",
    "    print ll_ratio_test[i]\n",
    "print \"\"\n",
    "for i in range(-5,0):\n",
    "    print ll_ratio_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to discuss/redo cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# print 'Training set'\n",
    "# print metrics.classification_report(y_train, y_pred_train)\n",
    "# print '\\nTest set'\n",
    "# print metrics.classification_report(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### optional ##########\n",
    "# # if import from sorted data - need to downsample train dataset to be more balanced\n",
    "# # can call the following function multiple times for decreasing in log term\n",
    "# # dropping PIF samples with 50% probability\n",
    "\n",
    "# def drop_half(train):\n",
    "#     print 'before drop: ', train.shape[0]\n",
    "#     dropidx = []\n",
    "#     for idx, val in train['LoanStatus'].iteritems():\n",
    "#         if val != 'CHGOFF':\n",
    "#             rand = np.random.rand()\n",
    "#             if rand < 0.5:\n",
    "#                 dropidx.append(idx)\n",
    "#     train = train.drop(dropidx)\n",
    "#     print 'after drop: ', train.shape[0]\n",
    "#     return train\n",
    "    \n",
    "# train = drop_half(train)\n",
    "# train = drop_half(train)\n",
    "# # test = drop_half(test)\n",
    "\n",
    "# chargeoff_count = (train['LoanStatus'] == \"CHGOFF\").sum()\n",
    "# print 'Chargeoffs:', chargeoff_count\n",
    "# print 'Percentage of loans that defaulted:', float(chargeoff_count)/len(train)\n",
    "\n",
    "# chargeoff_count = (test['LoanStatus'] == \"CHGOFF\").sum()\n",
    "# print 'Chargeoffs:', chargeoff_count\n",
    "# print 'Percentage of loans that defaulted:', float(chargeoff_count)/len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
